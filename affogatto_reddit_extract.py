# -*- coding: utf-8 -*-
"""Affogatto_Reddit_Extract.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GiP5uzR_BCOZmATLmt3ACZFNLbhm4BYg

# Script to Extract Reddit Posts
"""

#Pip Install
!pip install asyncpraw

!pip install --upgrade openai

#Import Necessary Libraries
import asyncpraw
import pandas as pd
import datetime as dt
import requests
from textblob import TextBlob
import nest_asyncio
import asyncio
import re
import spacy
import openai
from pydantic import BaseModel
from typing import Literal

#make sure openai version is at least 1.55
print(openai.__version__)

#build instance
reddit_read_only = asyncpraw.Reddit(client_id="CLIENT_ID",
                               client_secret="CLIENT_SECRET",
                               user_agent="USER_AGENT")

#make sure it can run in any environment
nest_asyncio.apply()

#get top n reddit posts
async def subreddit_top_n_posts(subreddit_name, n=150, time_frame="month"):
    subreddit = await reddit_read_only.subreddit(subreddit_name)
    posts = subreddit.top(time_frame, limit=n)

    posts_dict = {
        "Title": [],
        "Post Text": [],
        "ID": [],
        "Score": [],
        "Total Comments": [],
        "Post URL": []
    }

    count = 0
    async for post in posts:
        posts_dict["Title"].append(post.title)
        posts_dict["Post Text"].append(post.selftext)
        posts_dict["ID"].append(post.id)
        posts_dict["Score"].append(post.score)
        posts_dict["Total Comments"].append(post.num_comments)
        posts_dict["Post URL"].append(post.url)

        count += 1
        if count >= n:
            break

    top_posts = pd.DataFrame(posts_dict)
    return top_posts

#extract comments from a post link
async def get_comments_from_post(post_url):
    submission = await reddit_read_only.submission(url=post_url)
    await submission.load()

    post_comments = []
    for comment in submission.comments:
        if isinstance(comment, asyncpraw.models.MoreComments):
            continue
        post_comments.append(comment.body)

    comment_df = pd.DataFrame({'comment': post_comments,
                               'Post ID': submission.id})
    return comment_df

#get the comments from all posts in a list of post links
async def comments_from_all_posts(post_url_list):
    comments_df = pd.DataFrame()
    non_functional_links = []

    for post_url in post_url_list:
        try:
            post_df = await get_comments_from_post(post_url)
            comments_df = pd.concat([comments_df, post_df], ignore_index=True)
        except Exception as e:
            #links from which comments cannot be extracted
            non_functional_links.append(post_url)

    return comments_df, non_functional_links

#asynchronous execution function
async def main(subreddit_name, n=150, time_frame="month"):
    subreddit_name = subreddit_name
    top_posts = await subreddit_top_n_posts(subreddit_name, n, time_frame)
    post_urls = top_posts["Post URL"].tolist()
    comments_df, non_functional_links = await comments_from_all_posts(
        post_urls)

    return top_posts, comments_df, non_functional_links

#Preprocessing For Sentiment Analysis

#tokenize words
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

def tokenize(text):
    return word_tokenize(text)

#remove stopwords
from nltk.corpus import stopwords
nltk.download('stopwords')

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    return [word for word in text if word.lower() not in stop_words]

#lemmatize words
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

def lemmatize(text):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in text]

#string back together words
def one_string(text):
    return ' '.join(text)

#preprocessing function
def preprocess_text(text):
    tokens = tokenize(text)
    tokens = remove_stopwords(tokens)
    #tokens = lemmatize(tokens)
    return one_string(tokens)

#Sentiment Analysis
def sentiment_analysis_textblob(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

def sentiment_scorer(score):
    if score > 0.2:
        return 'positive'
    elif score < -0.2:
        return 'negative'
    else:
        return 'neutral'

def do_sentiment_analysis(text):
    score = sentiment_analysis_textblob(text)
    sentiment = sentiment_scorer(score)
    return sentiment

#load the NER model
nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'lemmatizer'])

def extract_place_names(text):
    doc = nlp(text)
    places = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC', 'FAC', 'ORG']]
    return places

def processing(top_posts,comments):
    #preprocess for each text column
    top_posts['Title_SentPre'] = top_posts['Title'].apply(preprocess_text)
    top_posts['Post_SentPre'] = top_posts['Post Text'].apply(preprocess_text)
    comments['comment_SentPre'] = comments['comment'].apply(preprocess_text)

    #sentiment analysis for each text column
    top_posts['Title_sentiment'] = top_posts['Title_SentPre'].apply(do_sentiment_analysis)
    top_posts['Post_sentiment'] = top_posts['Post_SentPre'].apply(do_sentiment_analysis)
    comments['comment_sentiment'] = comments['comment_SentPre'].apply(do_sentiment_analysis)

    #filter out only positive rows
    top_posts = top_posts[(top_posts['Title_sentiment'] == 'positive') | (top_posts['Post_sentiment'] == 'positive')]
    comments = comments[comments['comment_sentiment'] == 'positive']

    #extract placenames
    places_titles = top_posts['Title'].apply(extract_place_names)
    places_posts = top_posts['Post Text'].apply(extract_place_names)
    places_comments = comments['comment'].apply(extract_place_names)

    #extract all items to be single items
    places_titles = places_titles.explode().reset_index(drop=True)
    places_posts = places_posts.explode().reset_index(drop=True)
    places_comments = places_comments.explode().reset_index(drop=True)

    #merge the three lists
    places  = pd.concat([places_titles, places_posts, places_comments]).reset_index(drop=True)
    places = set(places)
    places = {x for x in places if x==x and x is not None}
    return places

def contains_emoticon(s):
    emoticon_pattern = re.compile(r'[:;=X][\-~]?[\)D\(\]/\\OpP]')
    return bool(emoticon_pattern.search(s))

def remove_emoticon(lst):
    return [x for x in lst if not contains_emoticon(x)]

#import csv file from local machine for nyc areas
from google.colab import files
uploaded = files.upload()

#extract data from .geojson file into a dataframe
import json

#extract all NYC neighborhoods and boroughs from file
with open('nyc-neighbourhood-data.geojson') as f:
    data = json.load(f)

neighborhoods = []
boroughs = []
for feature in data['features']:
    properties = feature['properties']
    neighborhoods.append(properties['neighborhood'])
    boroughs.append(properties['borough'])

nyc_areas = set(neighborhoods + boroughs)
nyc_areas = {x.lower() for x in nyc_areas}

#extract "neighborhood" and "borough" from data
def remove_nyc_areas(lst):
    return [x for x in lst if x.lower() not in nyc_areas]

#make sure it can run in any environment
nest_asyncio.apply()

async def places_from_subreddit(subreddit, n=150, time_frame="month"):
  top_posts, comments, non_functional_links = await main(subreddit, n, time_frame)
  places = processing(top_posts,comments)
  return places

def places_no_nonsense(places):
    places = remove_nyc_areas(places)
    places = remove_emoticon(places)
    return places

def final_places(subreddit):
  places = places_from_subreddit(subreddit)
  places = places_no_nonsense(places)
  return places

async def final_places_multiple_subreddits(subreddit_list, n=150, time_frame="month"):
  final_places = []
  subreddit_number = 0
  for subreddit in subreddit_list:
    places = await places_from_subreddit(subreddit, n, time_frame)
    places = places_no_nonsense(places)
    final_places.append(places)
    subreddit_number += 1
    print(f"{subreddit_number} out of {len(subreddit_list)} subreddits done")
  final_places = pd.Series(final_places).explode().reset_index(drop=True)
  return set(final_places)

subreddits = ['FoodNYC','AskNYC','bronx','Queens','Brooklyn','manhattan','statenisland','nyc']

final_places = asyncio.run(final_places_multiple_subreddits(subreddits, 300, "year"))

print(len(final_places))

print(final_places.iloc[:,1])

#save progress
pd.DataFrame(final_places).to_csv('final_places.csv')

#use chatgpt api to filter list - fill code
#filter out
#chain restaurants
#places that are not places
#places in New York

final_places = final_places.iloc[:,1].to_list()

print(len(list(final_places.iloc[:,1])))

api_key = 'openai_api_key'
client = openai.OpenAI(api_key=api_key)

def is_location_in_city(place_list,city):
  class Places(BaseModel):
    name: str
    real: bool
    #in_city: bool
    district: bool
    street: bool
    type: Literal['Athletics', 'Bar', 'Café', 'Cultural Immersion', 'Event', 'Museum', 'Nature', 'Restaurant']

  class PlacesResponse(BaseModel):
    choices: list[Places]

  completion = client.beta.chat.completions.parse(
      model="gpt-4o-2024-08-06",
      max_completion_tokens = 16000,
      messages=[
          {"role": "system", "content": ("You are a data analyst, who scraped data from Reddit to find the best places to be in a city.\n"
          "You take every place instance and you verify with online sources if it is a place that actually exists.\n"
          "Keep in mind that you haver a list of names and the following is your task"
          "You answer four questions\n"
          "1. Is this a place that actually exists or nonsense?\n"
          #"2. If it’s real, is it located in the given city?\n"
          "2. Is the place just a street name or a district within the city it id in (e.g. 23rd Stree or DeKalb avenue in NYC)"
          "For number 2, please do not mark district == True if the place is a park or a monument (e.g. Battery Park in NYC)"
          "3. Is the place a street name?"
          "4. What category of place is it?"
                                         )},
          {"role": "user", "content": f"Are the following {place_list} in {city}"}
      ],
      response_format=PlacesResponse,
  )

  filtered_places = completion.choices[0].message.parsed
  return filtered_places

def batch_location_filtering(places_list,city, batch_size=100):
  n = batch_size
  #divide list into m lists of length n
  all_filtered_places = []
  places_list_list = [places_list[j:j+n] for j in range(0, len(places_list), n)]
  count = 0
  for i in places_list_list:
    count += 1
    filtered_places = is_location_in_city(i,city)
    print(f'Round #{count} out of {len([[places_list_list]])} is done.')
    print(filtered_places.choices)
    all_filtered_places.append(filtered_places.choices)
  #all_filtered_places = [item for sublist in all_filtered_places for item in sublist]
  return all_filtered_places



filtered_places = batch_location_filtering(list(final_places.iloc[:,1]), 'New York City')

#check length of filtered list
print(len([i.name for i in filtered_places_explode if (i.real == True) & (i.district == False) & (i.street == False)]))

city_places = [i for i in filtered_places_explode if (i.real == True) & (i.district == False) & (i.street == False)]

#make city_places a dataframe
city_places = pd.DataFrame(city_places)

city_places.columns = ['name', 'real', 'district', 'street', 'category']
for c in city_places.columns:
  city_places[c] = city_places[c].apply(lambda x: x[1])
city_places.head()

#remove duplicates based on name
city_places = city_places.drop_duplicates(subset=['name'])

#sum city_places by categroy
city_places.groupby('category').count()

#Filter Out Fast Food Restaurants
fast_food_restaurants = [
    "McDonald’s",
    "Mickey D’s",
    "McD",
    "McDonalds",
    "Golden Arches",
    "Burger King",
    "BK",
    "King of Burgers",
    "Subway",
    "Subway Sandwiches",
    "Eat Fresh",
    "Taco Bell",
    "Taco Hell",
    "T-Bell",
    "Wendy’s",
    "Wendys",
    "Redhead",
    "KFC",
    "K-Fried",
    "Kentucky Chicken",
    "Colonel’s Chicken",
    "Chick-fil-A",
    "Chickfila",
    "Chick Fil A",
    "The Chicken Sandwich Place",
    "Domino’s Pizza",
    "Domino's",
    "Dominos Pizza",
    "Domino’s",
    "Pizza Hut",
    "The Hut",
    "PizzaHut",
    "Starbucks",
    "Bucks",
    "SBUX",
    "Starbucks Coffee",
    "Dunkin’",
    "Dunkin",
    "Dunkies",
    "DD",
    "Popeyes",
    "Popeyes Louisiana Kitchen",
    "Popeye’s Chicken",
    "Popeyes Chicken and Biscuits",
    "Sonic Drive-In",
    "Sonic",
    "Sonic Burger",
    "Arby’s",
    "Arbys",
    "Roast Beef Place",
    "Five Guys",
    "Five Guys Burgers and Fries",
    "FiveGuys",
    "In-N-Out Burger",
    "In-N-Out",
    "In and Out",
    "In & Out",
    "Jack in the Box",
    "Jack’s",
    "JITB",
    "Carl’s Jr.",
    "Hardee’s",
    "Carls Jr",
    "Little Caesars",
    "Little Caesar's",
    "Pizza! Pizza!",
    "Chipotle Mexican Grill",
    "Chipotle",
    "Chip"
]

city_places = city_places[~city_places['name'].isin(fast_food_restaurants)]

#export data
city_places.iloc[:,[0,4]].to_csv('city_places.csv')

"""## [To Be Implemented Later] NLP for better sentiment analysis to replace textblob"""

#tokenize words
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

def tokenize(text):
    return word_tokenize(text)

#remove stopwords
from nltk.corpus import stopwords
nltk.download('stopwords')

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    return [word for word in text if word.lower() not in stop_words]

#lemmatize words
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

def lemmatize(text):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in text]

#string back together words
def one_string(text):
    return ' '.join(text)

#cut off end of string if more than 510 tokens
def cut_string(text):
    if len(text) > 510:
        return text[:510]
    else:
        return text

#preprocessing function
def preprocess_text(text):
    tokens = tokenize(text)
    tokens = remove_stopwords(tokens)
    tokens = lemmatize(tokens)
    tokens = one_string(tokens)
    return = cut_string(tokens)

final_df['comment'] = final_df['comment'].apply(tokenize)
final_df['comment'] = final_df['comment'].apply(remove_stopwords)
final_df['comment'] = final_df['comment'].apply(lemmatize)
final_df['comment'] = final_df['comment'].apply(one_string)
final_df['comment'] = final_df['comment'].apply(cut_string)

final_df['Post Text'] = final_df['Post Text'].apply(tokenize)
final_df['Post Text'] = final_df['Post Text'].apply(remove_stopwords)
final_df['Post Text'] = final_df['Post Text'].apply(lemmatize)
final_df['Post Text'] = final_df['Post Text'].apply(one_string)
final_df['Post Text'] = final_df['Post Text'].apply(cut_string)

from transformers import pipeline
from joblib import Parallel, delayed

# Initialize the Hugging Face pipeline
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

# Function to process sentiment analysis in batches
def analyze_sentiment_batch(texts):
    # Use batch processing with the Hugging Face pipeline
    results = sentiment_pipeline(texts, batch_size=8)
    return [result['label'] for result in results]

# Function to split data into chunks
def chunkify(data, chunk_size):
    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]

# Parallelized processing function
def process_sentiment_parallel(data_column, chunk_size=100):
    # Split the data into chunks
    chunks = chunkify(data_column.tolist(), chunk_size)

    # Process chunks in parallel
    results = Parallel(n_jobs=-1)(
        delayed(analyze_sentiment_batch)(chunk) for chunk in chunks
    )

    # Flatten results from all chunks
    return [item for sublist in results for item in sublist]

# Example DataFrame
data = {
    "comment": [
        "I love this product!",
        "This is the worst experience ever.",
        "Python is amazing.",
        "I feel neutral about this.",
        "Absolutely terrible service!",
    ],
    "Post Text": [
        "The new update is fantastic.",
        "Why is the service so bad?",
        "Great features in the latest release.",
        "This is okay but could be better.",
        "Awful, I won't use this again."
    ]
}
#final_df = pd.DataFrame(data)

# Process sentiment for 'comment' and 'Post Text' columns
final_df['sentiment'] = process_sentiment_parallel(final_df['comment'], chunk_size=100)
final_df['post_sentiment'] = process_sentiment_parallel(final_df['Post Text'].unique(), chunk_size=100)

final_df.head(20)

x=final_df.loc[:,"comment"]

from transformers import pipeline
sentiment_pipeline = pipeline("sentiment-analysis",
                              model="distilbert-base-uncased-finetuned-sst-2-english")
def get_sentiment(text):
    return sentiment_pipeline(text,batch_size=8)[0]['label']

y = [get_sentiment(i) for i in x]

[type(i) for i in final_df['comment'] if not isinstance(i,str)]

set([type(i) for i in final_df['comment']])

#Sentiment Analysis
from transformers import pipeline
sentiment_pipeline = pipeline("sentiment-analysis",
                              model="distilbert-base-uncased-finetuned-sst-2-english")
def get_sentiment(text):
    return sentiment_pipeline(text,batch_size=8)[0]['label']

from joblib import Parallel, delayed

#results_sentiment = Parallel(n_jobs=-1)(delayed(analyze_sentiment)(text) for text in data)
final_df['sentiment'] = Parallel(n_jobs=-1)(delayed(get_sentiment)(text) for text in final_df['comment'])
final_df['post_sentiment'] = Parallel(n_jobs=-1)(delayed(get_sentiment)(text) for text in final_df['Post Text'].unique())
"""def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity"""

#final_df['sentiment'] = final_df['comment'].apply(get_sentiment)
#final_df['post_sentiment'] = final_df['Post Text'].apply(get_sentiment)

final_df

non_functional_links

comments_df

def subreddit_top_n_posts(subreddit_name,n=1000,time_frame="month"):
    posts = reddit_read_only.subreddit(subreddit_name).top(time_frame, limit=n)
    posts_dict = {"Title": [], "Post Text": [],
              "ID": [], "Score": [],
              "Total Comments": [], "Post URL": []
              }
    for post in posts:
      posts_dict["Title"].append(post.title)
      posts_dict["Post Text"].append(post.selftext)
      posts_dict["ID"].append(post.id)
      posts_dict["Score"].append(post.score)
      posts_dict["Total Comments"].append(post.num_comments)
      posts_dict["Post URL"].append(post.url)

    top_posts = pd.DataFrame(posts_dict)
    return top_posts

import asyncio

async def get_comments_from_post(post_url):
    submission = await reddit_read_only.submission(url=post_url) # Await the coroutine
    post_comments = []
    for comment in submission.comments:
        if type(comment) == asyncpraw.models.MoreComments:
            continue
        post_comments.append(comment.body)
    comment_df = pd.DataFrame({'comment': post_comments, 'Post ID': submission.id})
    return comment_df

# When calling the async function
async def main():  # Define an async main function
    comments_df = await get_comments_from_post(top_posts.iloc[0, -1])
    print(comments_df)


"""def get_comments_from_post(post_url):
    submission = reddit_read_only.submission(url=post_url)
    post_comments = []
    for comment in submission.comments:
        if type(comment) == asyncpraw.models.MoreComments:
            continue
        post_comments.append(comment.body)
    comment_df = pd.DataFrame({'comment': post_comments, 'Post ID': submission.id})
    return comment_df"""

"""def get_comments_from_post(post_url):
    submission = reddit_read_only.submission(url=post_url)
    post_comments = []

    # Ensure comments are fully loaded
    submission.comments.replace_more(limit=0)

    for comment in submission.comments:
        post_comments.append(comment.body)

    # Create a DataFrame with comments
    comment_df = pd.DataFrame({'comment': post_comments, 'Post ID': submission.id})
    return comment_df"""

"""def comments_from_all_post(post_url_list):
    comments_df = pd.DataFrame()
    non_functional_links = []
    for post_url in post_url_list:
        try:
            print(get_comments_from_post(post_url).head)
            comments_df = pd.concat([comments_df, get_comments_from_post(post_url)])
        except:
            non_functional_links.append(post_url)
            continue
    return comments_df, non_functional_links"""
def comments_from_all_posts(post_url_list):
    comments_df = pd.DataFrame()
    non_functional_links = []

    for post_url in post_url_list:
        try:
            # Fetch and concatenate comments
            post_df = get_comments_from_post(post_url)
            comments_df = pd.concat([comments_df, post_df], ignore_index=True)
        except Exception as e:
            # Log errors for debugging
            print(f"Error with URL {post_url}: {e}")
            non_functional_links.append(post_url)
            continue

    return comments_df, non_functional_links

def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity



#function to store all text in one table
#def store_all_text(df):

import nest_asyncio
nest_asyncio.apply()

# Run the async function using asyncio.run()
asyncio.run(main())

get_comments_from_post(top_posts.iloc[0,-1])

comments_df, non_functional_links = comments_from_all_post(top_posts.iloc[:,-1])
comments_df

comments_df

#get posts from a subreddit
sr_name = "FoodNYC"

posts = subreddit_top_n_posts(sr_name)

#get

subreddit = reddit_read_only.subreddit("FoodNYC")

# Display the name of the Subreddit
print("Display Name:", subreddit.display_name)

# Display the title of the Subreddit
print("Title:", subreddit.title)

#extract posts
for post in subreddit.top(limit=5):
    print(post.title)
    print()

posts = subreddit.top("month")
# Scraping the top posts of the current month

posts_dict = {"Title": [], "Post Text": [],
              "ID": [], "Score": [],
              "Total Comments": [], "Post URL": []
              }

for post in posts:
    # Title of each post
    posts_dict["Title"].append(post.title)

    # Text inside a post
    posts_dict["Post Text"].append(post.selftext)

    # Unique ID of each post
    posts_dict["ID"].append(post.id)

    # The score of a post
    posts_dict["Score"].append(post.score)

    # Total number of comments inside the post
    posts_dict["Total Comments"].append(post.num_comments)

    # URL of each post
    posts_dict["Post URL"].append(post.url)

# Saving the data in a pandas dataframe
top_posts = pd.DataFrame(posts_dict)
top_posts

url = top_posts.iloc[0,-1]

# Creating a submission object
submission = reddit_read_only.submission(url=url)


post_comments = []

for comment in submission.comments:
    if type(comment) == MoreComments:
        continue

    post_comments.append(comment.body)

# creating a dataframe
comments_df = pd.DataFrame(post_comments, columns=['comment'])
comments_df

submission.id

POST /api/search_subreddits

#sentiment analysis of comments_df
from textblob import TextBlob

def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

comments_df['sentiment'] = comments_df['comment'].apply(get_sentiment)

comments_df

